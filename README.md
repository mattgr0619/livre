# Music Classification with Convolution

It is without a doubt that the AI revolution is upon us. From Google’s AlphaFold uncovering the human genome to optimized recommendation systems that improve companies such as Airbnb and Amazon, we certainly are not going to be hearing about it anytime soon.

Perhaps one of the biggest areas where AI is taking over (or maybe better characterized as ML -- see differences here), is deep learning. What is deep learning?

Deep learning, simply stated, is an advanced type of data analysis that originated in the 1940’s and is attributed to Alan Turing and Walter Pitts, among many other burgeoning mathematicians and engineers. This type of analysis allows you to take an array of data (oftentimes an image), and reduce its dimensionality to a classification (the classic example is classifying images as either cats or dogs). These algorithms are exciting, as they can solve complex issues, like serving as the “eyes” of a self-driving car (think image classification as either “drive that way” or ”dont drive that way”). Aso exciting, although such AI capabilities are being scaled at the mass department/lab/company-wide level, the great thing is that it can be scaled and tinkered with at the individual level -- and for free!

One thing I recently found out is that the algorithms are also being used to categorize sound and music. By converting a sound into a spectrogram, or essentially a three dimensional frequency (y) and amplitude (color) over time (x) graph (look below too for an example), these algorithms are able to approach sound the way they approach sight -- as an array! 

Here is an example of a spectrogram:

